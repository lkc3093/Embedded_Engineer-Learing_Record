## ARM NEON

````c
	asm volatile (
		"1: \n"                        // 用于构成循环的标记号
		"vld1.32 {q0}, [%[src1]]! \n"  // 从src地址处载入4个32位的浮点数 地址递增
		"vld1.32 {q1}, [%[src2]]! \n"
		"vadd.f32 q0, q0, q1 \n"       // q0 = q0 +q1 四位32的浮点数递增
		"subs %[count], %[count], #4 \n"// 循环计数count = count-4
		"vst1.32 {q0}, [%[dst]]! \n"   // 将运算结果存储到目标地址，目标地址递增
		"bgt 1b \n"                    // 如果count>0,跳转到标记号1处继续执行
		: [dst] "+r" (dst)             // 可写
		: [src1] "r" (src1), [src2] "r" (src2), [count] "r" (count)
		: "memory", "q0", "q1"
	);
````



```c
push {r4-r7}                      @保存r4-r7寄存器的内容
vdup.8 q0,r0                      @u 从r0里面取出第一个参数（u）放到q0寄存器 寄存器到寄存器
vld1.8 {q1},[r1]!                 @v 从r1里面取出第一个参数（v）放到q1寄存器 内存到寄存器
vabd.s8 q2,q0,q1                  @abs(u-v)   求 | u-v |，值存放到寄存器 q2
vld3.8 {d0-d2}, [r1]!             @从r1开始加载3个字节到d1,d2,d3
vmull.u8 q3, d0, d3               @将d0内的值和d3内的值相乘后存储在q3里 
vmlal.u8 q3, d2, d5               @将d2内的值和d5内的值相乘的结果累加到q3内的值
vshrn.u16 d6, q3, #8              @将q3除以256后值存储在d6里
 vst1.8 {d6}, [r0]!               @将d6内的值（一个字节）存储到地址r0开始的地方，并将r0的地址加1
subs r2, r2, #1                   @将r2的值减1后再赋给r2，如果结果为0，就改变状态位，那么跳转的时候就可以根据这个状态值来做出是否跳转的决定
bne loop                          @如果上面的结果为0，那么ne的结果就为假，程序就不跳转到loop，也就是跳出循环
push {r4-r5,lr}                   @将r4,r5,lr进栈保存起来
pop {r4-r5, pc}                 @恢复之前保存的寄存器r4,r5，程序跳转到下一条指令执行。
vld1.8 {d0,d1,d2},[r1]!           @从r1里面取出8个值存在d0，然后再取8个放在d1，然后再取8个放在d2，这叫做线性存储
vld3.8 {d0,d1,d2}, [r1]!          @从r1取3个字节分别放到d0,d1,d2，接着再取3个字节放到d0,d1,d2,如此反复下去，这叫结构性存储，多见于rgb

```



```c
neon支持的主要操作

助记符    含义
add    加法
sub    减法
mul    乘法
mla    乘加
mls    乘减
ceq    比较 ==
cge    比较 >=
cle    比较 <=
cgt    比较 >
clt    比较 <
max    最大值
min    最小值
shl    左移位
shr    右移位
abs    求绝对值
neg    取反
mvn    按位取反
and    与运算
orr    或运算
eor    异或运算
get    取值
set    赋值
dup    构造一个向量，并进行初始化
combine    合并操作，将两个向量进行合并
mov    改变数据类型和数据范围
zip    压缩操作
uzp    解压操作
ld1    加载数据，从给定的buffer指针中拷贝数据
st1    拷贝数据，将数据拷贝到指定的buffer中
tst    进行与运算之后，判断是否等于0
abd    两个向量相减之后的绝对值
```

* `{}` 内是要传入的寄存器

* 矢量寄存器可以用 `d`，`q` 寄存器同时表示
  * `q` 代表**4X32** bits
  * `d` 代表**4X16** bits

* `!` 表示执行完指令后，基地址自加

* **VFP** 指令是矢量浮点指令

  

## **RISCV VECTOR**

* 可变长的矢量寄存器，通过 `VLEN` 和 `SLEN` 参数来控制
  * `VLEM` 是矢量寄存器的位数
  * `SEW` 是单个变量所占的寄存器位数

* 可以交织读取内存中的值
* **RISCV** 可以使用矢量向量寄存器组，一次性处理多个向量寄存器数据；由 `LMUL` 决定使用的向量寄存器的个数，也可以是分数，表示只使用单个向量寄存器的高位

<br>

````c
  asm volatile(
               "mv         t4,   %[LEN]       \n\t"
               "mv         t1,   %[PA]        \n\t"
               "mv         t2,   %[PB]        \n\t"
               "mv         t3,   %[PC]        \n\t"
               "LOOP1:                        \n\t"
               "vsetvli    t0,   t4,   e32,m1 \n\t"
               "sub        t4,   t4,   t0     \n\t"
               "slli       t0,   t0,   2      \n\t" //Multiply number done by 4 bytes
               "vle.v      v0,   (t1)         \n\t"
               "add        t1,   t1,   t0     \n\t"
               "vle.v      v1,   (t2)         \n\t"
               "add        t2,   t2,   t0     \n\t"
               "vfadd.vv   v2,   v0,   v1     \n\t" // 浮点计算单元
               "vse.v      v2,   (t3)         \n\t"
               "add        t3,   t3,   t0     \n\t"
               "bnez       t4,   LOOP1        \n\t"
               :
               :[LEN]"r"(len), [PA]"r"(a),[PB]"r"(b),[PC]"r"(c)
               :"cc","memory", "t0", "t1", "t2", "t3", "t4",
                "v0", "v1", "v2"
               );
````


* `m1` 表示 `LMUL` 只使用1个向量寄存器；`e32` 代表单个向量的位数；`t0` 是返回的向量个数值
* 比如通用向量寄存器一共32个，设置`LMUL`为8，就可以将全部的通用寄存器分成4组，每组包含了8个连续的向量寄存器
* `t4` 是需要计算的所有向量的长度；`vse` 是向量写回指令；`vle` 是向量读取指令
* `vsetvli` 指令需要在所有向量指令最前
* `vmul`:  控制多个vector合并为一个更大的vector，得到更少但是更长的向量寄存器，被允许的向量名字必须是偶数名字，例如：`vmul = m2 {v0,v2,v4…}, vmul = m4 {v0,v4,v8…} ， vmul = 1 {v0， v1， v2~v30}`；可取的数值：`m1， m2， m3 ~ m8`

<br>

````c
void riscv_cmplx_conj_q15(
  const q15_t * pSrc,
        q15_t * pDst,
        uint32_t numSamples)
{
#if defined(RISCV_VECTOR)
  uint32_t blkCnt = numSamples;                               /* Loop counter */
  size_t l;
  l = vsetvl_e32m8(blkCnt);
  const q15_t * input = pSrc;
  q15_t * output = pDst;
  ptrdiff_t bstride = 4;
  vint16m8_t v_R,v_I;
  vint16m8_t v0;
  v0 = vxor_vv_i16m8(v0, v0, l);                   /* vector 0 */
  for (; (l = vsetvl_e16m8(blkCnt)) > 0; blkCnt -= l)  // 返回处理的长度
  {
    v_R = vlse16_v_i16m8(input, bstride, l);
    input++;
    vsse16_v_i16m8 (output, bstride, v_R, l);
    output++;
    v_I = vsub_vv_i16m8(v0, vlse16_v_i16m8(input, bstride, l), l);
    input += (l*2-1);
    vsse16_v_i16m8 (output, bstride, v_I, l);
    output += (l*2-1);
  }
````

````c
    # Vector strided loads and stores

    # vd destination, rs1 base address, rs2 byte stride
    vlse8.v    vd, (rs1), rs2, vm  #    8-bit strided load
    vlse16.v   vd, (rs1), rs2, vm  #   16-bit strided load
    vlse32.v   vd, (rs1), rs2, vm  #   32-bit strided load
    vlse64.v   vd, (rs1), rs2, vm  #   64-bit strided load
    vlse128.v  vd, (rs1), rs2, vm  #  128-bit strided load
    vlse256.v  vd, (rs1), rs2, vm  #  256-bit strided load
    vlse512.v  vd, (rs1), rs2, vm  #  512-bit strided load
    vlse1024.v vd, (rs1), rs2, vm  # 1024-bit strided load

    # vs3 store data, rs1 base address, rs2 byte stride
    vsse8.v    vs3, (rs1), rs2, vm  #    8-bit strided store
    vsse16.v   vs3, (rs1), rs2, vm  #   16-bit strided store
    vsse32.v   vs3, (rs1), rs2, vm  #   32-bit strided store
    vsse64.v   vs3, (rs1), rs2, vm  #   64-bit strided store
    vsse128.v  vs3, (rs1), rs2, vm  #  128-bit strided store
    vsse256.v  vs3, (rs1), rs2, vm  #  256-bit strided store
    vsse512.v  vs3, (rs1), rs2, vm  #  512-bit strided store
    vsse1024.v vs3, (rs1), rs2, vm  # 1024-bit strided store
````

* `bstride` 用于交织读取

  

## DSP
* 基本都是 **VILW** 指令架构，类似于单核多发射，但是对编译器要求更高( 指令重排都是由编译器去完成 )
* 基本都是哈佛架构，指令总线与数据总线独立，能并行读取指令和数据

### Hexagon
* 支持硬件多线程 (相当于多核心)
* 改进版的 **VILW** 指令集，能实现发射依赖指令，数据依赖的指令包也能发射出去( 应该是有一个特殊的程序计数器 )

### Candence hifi / vision

*  大的  **SIMD** 指令位宽
*  增加了FP16的操作以及寄存器
*  五路 **VILW** 指令槽
*  低功耗

# **openmp**

````c
float a[1024];
float b[1024];
float c[1024];
int size;

void vadd_openmp(float *a, float *b, float *c, int size)
{
    // map(to:a[0:size], b[0:size], size)) 表示数据由主机内存-->设备内存
    // map(from: c[0:size]) 表示数据由设备内存-->主机内存, 也就是存入主机内存
    #pragma omp target map(to:a[0:size],b[0:size],size) map(from: c[0:size]) 
    {
        int i;
        #pragma omp parallel for // 将size个循环分到多个核心上运行
        for (i = 0; i < size; i++)
            c[i] = a[i] + b[i];

    }
}


````



* **barrier** 控制线程间的同步, **nowait** 取消同步

```c
  /*这个程序中的第一个for循环同样会有多个线程同时执行，只是其中某个线程最先执行完了之后，
	  不会等其他的线程，而是直接进入了下一个for循环，所以结果中的 - 和 + 在中间部分是混杂的。*/
	  #pragma omp parallel 
	  {
	      #pragma omp for nowait
	      for (int i = 0; i < 100; i++) 
		  {
	          cout << "+" ;
	      }

          #pragma omp for 
              for (int j = 666; j < 1000; j++) 
              {
                  cout << "-" ;
              }
          }

  /*可知，barrier为隐式栅障，即并行区域中所有线程执行完毕之后，主线程才继续执行。
  而nowait的声明即可取消栅障，这样，即使并行区域内即使所有的线程还没有执行完，
  但是执行完了的线程也不必等待所有线程执行结束，而可自动向下执行。*/

  printf("\n\n\n");
 /* 如下所示正常来说应该是第一个for循环中的一个线程执行完之后nowait进入下一个for循环，
  但是我们通过 #pragma omp barrier 来作为显示同步栅障，即让这个先执行完的线程等待所有线程执行完毕再进行下面的运算*/

  #pragma omp parallel 
  {
      #pragma omp for nowait
      for (int i = 0; i < 100; i++) 
	  {
          cout << "+" ;
      }

      #pragma omp barrier//等待for循环中所有线程运行结束，再运行下面的

      #pragma omp for  // 多个线程运行
      for (int j = 666; j < 1000; j++) 
	  {
          cout << "-" ;
      }
  }
```


![image-20220223101943019](/home/liangkc/github/Embedded_Engineer-Learing_Record/work_recoder/img/image-20220223101943019.png)



* **#pragma omp master** 单线程运行某一段代码，其他的段代码分到其他线程并行运行

```c
  /*这里我们通过#pragma omp master来让主线程执行for循环，然后其他的线程执行后面的cout语句，
  所以，cout的内容会出现在for循环多次（这取决于你电脑的性能），最后，主线程执行完for语句后，也会执行一次cout*/
  #pragma omp parallel
  {
      #pragma omp master/* 单个线程执行 */ 
      {
          for (int i = 0; i < 10; i++) 
		  {
              cout << i << endl;
          }
      }
      cout << "This will be shown two or more times\n" << endl; // 多线程执行
  }
```



![image-20220223101529713](/home/liangkc/github/Embedded_Engineer-Learing_Record/work_recoder/img/image-20220223101529713.png)

* **#pragma omp parallel sections——指定部分的代码运行在特定线程**

```c
  /*使用section可以指定不同的线程来执行不同的部分
	  如下所示，通过#pragma omp parallel sections来指定不同的section由不同的线程执行
	  最后得到的结果是多个for循环是混杂在一起的*/
	  #pragma omp parallel sections
	  {
	      #pragma omp section
	      for (int i = 0; i < 10; i++) 
		  {
	          cout << "+";
	      }
      
	  #pragma omp section 
      for (int j = 0; j < 10; j++) 
	  {
          cout << "-";
      }

      #pragma omp section
      for (int k = 0; k < 10; k++) 
	  {
          cout << "*";
      }        
  }
```


![嵌入式算法移植优化学习笔记1——openmp（多核编程框架）_分享_10](/home/liangkc/github/Embedded_Engineer-Learing_Record/work_recoder/img/1)


* 基本语法

  ```c
  #pragma omp 指令 [子句 [ [ [,]子句 ] ... ]
  
  {
  
  ...
  
  }
  ```

* 调度

  * 静态调度	

    * 没有带 **schedule**语句时，默认都是静态调度
  * 动态调度，以及启发性调度，都是将计算迭代分配
  * 不管是静态还是动态调度，**for** 循环的并行都是编译器自动计算出迭代的条件然后分配给不同线程，不同的线程只需要关心 **for** 循环里面的计算内容即可

* 可以指定核心线程去执行特定的代码

* **openmp **在处理多级并行嵌套时默认采用串行的执行方式， 效率较低， 不建议使用

* 全局变量的竞争可以使用 **reduction**， **critical**， **atomic** 等线程内同步方式解决

  
  * **atomic** 只能用在计算语句上，不是所有语句都能使用原子操作
  * 全局变量的竞争会导致很多不可知的情况出现，目前 **openmp** 默认还没能解决该问题
  

