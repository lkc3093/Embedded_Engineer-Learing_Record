## CUDA

* 一个**SM** 一个时刻只能运行一个 **warp**，但是可以有多个warp停驻在一个 **SM** 里面

  * 一旦其中一个 **warp** 发生IO操作，立刻切换到下一个 **active wrap**

* **block** 可以设置 **warp** 的数量，最好

* **CUDA** 是有 **cache** 的概念，指令流水线；最新的架构甚至有多个的指令流水线( 按指令分类 )

  * 数据从全局内存到SM（**stream-multiprocessor**）的传输，会进行cache，如果 **cache **命中了，下一次的访问的耗时将大大减少。
    每个SM都具有单独的L1 cache，所有的SM共用一个L2 cache
    在计算能力2.x之前的设备，全局内存和局部内存的访问都会在 **L1\L2 cache** 上缓存；在计算能力3.x以上的设备，全局内存的访问只在**L2 cache**上缓存

    

* 访问连续的全局内存时，**GPU** 会合并访问，一次取出多个连续数据

  * 合并访问是指所有线程访问连续的对齐的内存块，对于L1 cache，内存块大小支持32字节、64字节以及128字节，分别表示线程束中每个线程以一个字节（1 * 32=32）、16位（2 * 32=64）、32位（4 * 32=128）为单位读取数据。前提是，访问必须连续，并且访问的地址是以32字节对齐。（类似于SSE\AVX的向量指令，**cuda**  中的合并访存也是向量指令）

    例子，假设每个 **thread **读取一个float变量，那么一个**warp**（32个**thread**）将会执行32*4=128字节的合并访存指令，通过一次访存操作完成所有thread的读取请求

  * **Shared Memory**

    * 线程块内共享的

    * 访问共享内存时，可能会出现 **bank** **conflict**，局部内存是一个个 **bank** 组成

      ​	__shared __修饰符修饰的变量存放在shared memory。因为 shared memory 是on-chip的，他相比 localMemory 和 global memory 来说，拥有高的多	bandwidth和低很多的latency。他的使用和CPU的 L1cache 非常类似，但是他是 programmable 的

      ​	按惯例，像这类性能这么好的memory都是有限制的，shared memory是以block为单位分配的；我们必须非常小心的使用shared memory，否则会	无意识的限制了active warp的数目

    * 要解决bank冲突，首先我们要了解一下共享内存的地址映射方式。
      在共享内存中，连续的32-bits字被分配到连续的32个bank中，这就像电影院的座位一样：一列的座位就相当于一个bank，所以每行有32个座位，在每个座位上可以“坐”一个32-bits的数据(或者多个小于32-bits的数据，如4个`char`型的数据，2个`short`型的数据)；而正常情况下，我们是按照先坐完一行再坐下一行的顺序来坐座位的，在shared memory中地址映射的方式也是这样的。下图中内存地址是按照箭头的方向依次映射的：

      

      ![bank-layout](https://segmentfault.com/img/bVFMd3)

      

      上图中数字为bank编号。这样的话，如果你将申请一个共享内存数组(假设是int类型)的话，那么你的每个元素所对应的bank编号就是地址偏移量(也就是数组下标)对32取余所得的结果，比如大小为1024的一维数组myShMem:

      - myShMem[4]: 对应的bank id为#4 (相应的行偏移量为0)

      - myShMem[31]: 对应的bank id为#31 (相应的行偏移量为0)

      - myShMem[50]: 对应的bank id为#18 (相应的行偏移量为1)

      - myShMem[128]: 对应的bank id为#0 (相应的行偏移量为4)

      - myShMem[178]: 对应的bank id为#18 (相应的行偏移量为5)

        

  * **CUDA array** 在 cuda 中是一个特殊的类型，叫做 cudaArray，在 CUDA 中，他是专门给 texture 用的一种数组；通过cudaMallocArray()、cudaFreeArray()、cudaMemcpyToArray()等函数对其进行管理。此外，由于 cudaArray 本身不是template 的型別，所以在通过cudaMallocArray()来申请CUDA Array时，要通过cudaChannelFormatDesc这个特别的方式，来定义CUDA Array的类型

  * **Constant Memory**

    常量内存同样是offchip内存，只读，拥有SM私有的 **constant cache**，因此在cache hit的情况下速度快。常量内存是全局的，对所有 Kernel 函数可见。因此声明要在Kernel函数外

  * **local memory** 是线程私有的，寄存器不够，就会将数据存入 **local memory**

* **CUDA** 可以使用洗牌指令实现线程内的数据交互，可以通过归约操作来完成一些累加的算法计算

  * 由于**CUDA** 的**warp** 执行的是同一条指令，导致会有部分是重复运算


* 当 **block** 中的线程数超过 **SM** 的上限，会导致 kernel 不运行；目前英伟达官方将块的上限设置为1024个线程，似乎线程有对应的硬件概念
* **dispatch port** 是指令发送端口，如果功能单元正在使用，**dispatch port** 就会一直被占用，导致其他指令无法发射；所以一个 **dispatch unit** 可以根据不同的功能单元，配置多个 **dispatch port** ，保证指令的吞吐

  * 属于流水线的结构冲突
* 分支处理

  * 会将两个分支都执行一次，分支的入口会存在 **warp stack** 上，但是不符合条件的会通过 **mask** 来规避
  * **volta** 以及之后的架构这时候会执行的分支的乱序执行
  * **for** 循环也算是分支
* 动态分配 **shared_memory**, 可以在 **kernel** 分配不定内存的数组


------

> 指令执行吞吐一般指的是每个时钟周期内可以执行的指令数目，不同指令的吞吐会有所不同。通常GPU的指令吞吐用每个SM每周期可以执行多少指令来计量。对于多数算术逻辑指令而言，指令执行吞吐只与SM内的单元有关，整个GPU的吞吐就是每个SM的吞吐乘以SM的数目。而GPU的FMA指令（通常以F32计算）往往具有最高的指令吞吐，其他指令吞吐可能与FMA吞吐一样，或是只有一半、四分之一等等。所以很多英文文档会说FMA这种是full throughput，一半吞吐的是half rate，四分之一的是quarter rate等。当然，有些微架构下也会有1/3、1/6之类非2的幂次的比率

* 指令吞吐不仅与指令类型有关，还与微架构具体设计实现有关。它主要会受到以下一些因素的影响：

  1. **功能单元**的数目。绝大多数指令的功能都需要专用或共享的硬件资源去实现，设计上配置的功能单元多，指令执行的吞吐才可能大。显然，只有最常用的那些指令，才能得到最充分的硬件资源。而为了节约面积，很多指令的功能单元会相互共享，所以他们的吞吐往往也会趋于一致。比如浮点的FFMA、FMUL都要用到一个至少24bit的整数乘法器（32bit浮点数有23bit尾数，小数点前还有1bit）。以前一些处理器有24bit的整数乘法指令，两者乘法器就可以共用，从而具有相同的吞吐（不过NV最近几代好像都没有这个指令，ptx以及内置函数的24bit乘法应该是多个指令模拟的）。而FADD虽然用不上那个乘法器，但可以与FFMA共用那个很宽的加法器，以及一些通用的浮点操作（特殊数的处理，subnormal flush之类）。32bit的整数乘法因为需要更宽的乘法器，有的就不会做成full throughput，甚至可能被拆分成多个指令（比如Maxwell和Pascal用三个16bit乘法指令XMAD完成一次32bit整数乘法）。Turing的

     IMAD应该是有意识的加宽了，所以32bit的IMAD与FFMA吞吐一样，但印象中带64bit加数的IMAD应该还是一半。再比如一些超越函数指令（MUFU类，比如rcp，rsq，sin，exp之类），由于实际使用量相对不会太频繁，多数是1/4的throughput

     

  2. 指令 **Dispatch Port** 和 **Dispatch Unit** 的吞吐。这个在之前的专栏文章也详细讲过。一个warp的指令要发射，**首先要eligible**，也就是不要因为各种原因stall，比如指令cache miss，constant immediate的miss，scoreboard未就位，主动设置了stall count等等。**其次要被warp scheduler选中**，由Dispatch Unit发送到相应的Dispatch Port上去。Kepler、Maxwell和Pascal是一个Warp Scheduler有两个Dispatch Unit，所以每cycle最多可以发射两个指令，也就是双发射。而Turing、Ampere每个Warp Scheduler只有一个Dispatch Unit，没有双发射，那每个周期就最多只能发一个指令。但是Kepler、Maxwell和Pascal都是一个Scheduler带32个单元（这里指full-throughput的单元），每周期都可以发新的warp。而Turing、Ampere是一个Scheduler带16个单元，每个指令要发两cycle，从而空出另一个cycle给别的指令用。**最后要求Dispatch Port或其他资源不被占用**，port被占的原因可能是前一个指令的执行吞吐小于发射吞吐，导致要Dispatch多次，比如Turing的两个FFMA至少要stall 2cycle，LDG之类的指令至少是4cycle。更详细的介绍大家可以参考之前的专栏文章

     

  3. **GPR读写吞吐**。绝大部分的指令都要涉及GPR的读写，由于Register File每个bank每个cycle的吞吐是有限的（一般是32bit），如果一个指令读取的GPR过多或是GPR之间有bank conflict，都会导致指令吞吐受影响。GPR的吞吐设计是影响指令发射的重要原因之一，有的时候甚至占主导地位，功能单元的数目配置会根据它和指令集功能的设计来定。比如NV常用的配置是4个Bank，每个bank每个周期可以输出一个32bit的GPR。这样FFMA这种指令就是3输入1输出，在没有bank conflict的时候可以一个cycle读完。其他如DFMA、HFMA2指令也会根据实际的输入输出需求，进行功能单元的配置

     

  4. 很多指令有**replay**的逻辑（[参考Greg Smith在StackOverflow上的一个回答](http://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/35566178/how-to-explain-instruction-replay-in-cuda)）。这就意味着有的指令一次发射可能不够。这并不是之前提过的由于功能单元少而连续占用多轮dispath port，而是指令处理的逻辑上有需要分批或是多次处理的部分。比如constant memory做立即数时的cache miss，memory load时的地址分散，shared memory的bank conflict，atomic的地址conflict，甚至是普通的cache miss或是TLB的miss之类。根据上面Greg的介绍，Maxwell之前，这些replay都是在warp scheduler里做的，maxwell开始将它们下放到了各级功能单元，从而节约最上层的发射吞吐。不过，只要有replay，相应dispath port的占用应该是必然的，这样同类指令的总发射和执行吞吐自然也就会受影响

* **静态资源分配**

  * GPU有一个很重要的设计逻辑是尽量减少硬件需要动态判断的部分。GPU的每个线程和block运行所需的资源尽量在编译期就确定好，在每个block运行开始前就分配完成（Block是GPU进行运行资源分配的单元，也是计算Occupancy的基础）。典型的运行资源有GPR和shared memory。GPU程序运行过程中，一般也不会申请和释放内存（当然，现在有device runtime可以在kernel内malloc和free，供Dynamic Parallelism用，但这个不影响当前kernel能用的资源）。CPU在运行过程中有很多所需的资源是动态调度的。比如，x86由于继承了祖上编码的限制，ISA的GPR数目往往比物理GPR少，导致常常出现资源冲突造成假依赖。实际运行过程中，通常会有register renaming将这些ISA GPR映射到不同的物理GPR，从而减少依赖（有兴趣的同学可以研究下tomasulo算法）。GPU没有这种动态映射逻辑，每个线程的GPR将一一映射到物理GPR。由于每个线程能用的GPR通常较多，加上编译器的指令调度优化，这种假依赖对性能的影响通常可以降到很低的程度

    

    每个block在运行前还会分配相应的shared memory，这也是静态的。这里需要明确的是，每个block的shared memory包括两部分，写kernel时固定长度的静态shared memory，以及启动kernel时才指定大小的动态shared memory。虽然这里也分动静态，但指的是编译期是否确定大小，在运行时总大小在kernel启动时已经确定了，kernel运行过程中是不能改变的

    

    其实block还有一些静态资源，比如用来做block同步的barrier，每个block最多可以有16个。我暂时没测试到barrier的数目对Occupancy的影响，也许每个block都可以用16个。另一种是Turing后才出现的warp内的标量寄存器Uniform Register，每个warp 63个+恒零的URZ。因为每个warp都可以分配到足额，应该对Occupancy也没有影响。另外每个线程有7个predicate，每个warp有7个Uniform predicate，这些也是足额，也不影响Occupancy

    

    GPU里还有一种半静态的stack资源，通常也可以认为是thread private memory或者叫local memory。多数情况下每个线程会用多少local memory也是确定的。不过，如果出现一些把local memory当stack使用的复杂递归操作，可能造成local memory的大小在编译期未知。这种情况编译器会报warning，但是也能运行。不过local memory有最大尺寸限制，当前是每个线程最多512KB（参考[CUDA C Programming Guide, Table 15](http://link.zhihu.com/?target=https%3A//docs.nvidia.com/cuda/cuda-c-programming-guide/index.html%23features-and-technical-specifications__technical-specifications-per-compute-capability), Maximum amount of local memory per thread=512KB）

  * **顺序执行**：乱序执行是CPU提高CPI的一个重要途径，但乱序执行无论是设计复杂度还是运行控制的开销都很大。CPU的乱序执行可以把一些不相关的任务提前（相关的也可以乱序，但要求顺序提交），从而提高指令并行度，降低延迟。而GPU主要通过Warp切换的逻辑保持功能单元的吞吐处于高效利用状态，这样总体性能对单个warp内是否stall就不太敏感

    虽然GPU一般是顺序执行，但指令之间不相互依赖的时候，可以连续发射而不用等待前一条指令完成。在理想的情况下，一个warp就可以把指令吞吐用满。当然，实际程序还是会不可避免出现stall（比如branch），这时就需要靠TLP来隐藏这部分延迟

------

### Kernel Parallel

* 通过 **stream** 可以实现多个内核计算并行，或者计算与IO并行；流与流之间是并发关系，类似于**opencl** 中的命令队列( 乱序 )

我们知道在一个stream中，kernel的执行顺序一定是串行的，由于算力3.5以上的设备基本都支持了kernel并行，所以为了最大程度利用好GPU的资源，我们可以利用多个流来实现kernel并行的效果，进而不断提升性能，榨干GPU的资源。

### 原理概述

首先我们要把一些任务塞stream中，stream的任务然后再次被转到硬件的任务队列里，GPU在运行的时候最后是通过队列调度器来执行。

### 深度优先

说个大白话就是，一个流的任务添加完后再添加下一个流的任务。下面举一个简单的例子，比如我们在CPU端设置了3个stream, 第一个流A->B->C, 第二个流是O->P->Q, 第三流是X->Y->Z; 假如只有一个GPU的硬件工作队列，那么伪代码如下：

```
### CPU端
stream_1->add(C);
stream_1->add(B);
stream_1->add(A);
stream_2->add(Q);
stream_2->add(P);
stream_2->add(O);
stream_3->add(Z);
stream_3->add(Y);
stream_3->add(X);
### GPU端
quene->add(C,B,A,Q,P,O,Z,Y,X)
```

stream中图示如下：

![在这里插入图片描述](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzFjNDc2MzQ4NTMyYjQ0ZTA5MmZlZTBiMGIwYTM3MThlLnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTYjcGljX2NlbnRlcg==)
硬件队列图示如下：

![深度优先](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzZiMzlmZTYxYTA3YjQ0YjI4M2UyN2VlZWEyOTlmMTdiLnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTYjcGljX2NlbnRlcg==)

### 广度优先

说个大白话就是，每个流轮流添加任务。伪代码如下：

```
stream_1->add(C);
stream_2->add(Q);
stream_3->add(Z);
stream_1->add(B);
stream_2->add(P);
stream_3->add(Y);
stream_1->add(A);
stream_2->add(O);
stream_3->add(X);
#quene->add(C,Q,Z,B,P,Y,A,O,X)
```

![广度优先](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzczNTc4M2JmYmIxZTQ1YThiMDlkMTUzZTU3NTY1Y2U4LnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTYjcGljX2NlbnRlcg==)

### 深度优先性能

当利用深度优先的时候，GPU的队列调度器会首先去取任务C部署在GPU上，当发现还有很多计算资源的时候，然后去取B任务，但是发现B依赖的是C, 而C还没计算完成，那么就会等C返回后才能开始部署B，这样就导致真实的GPU运行如下, 重叠部分是由于当在部署A的时候，发现还有计算资源，然后就去取Q, 发现Q没有依赖，所以就可以同时运行，但是即使还有计算资源剩余，P也不能执行，因为他所依赖的Q还没返回。

![在这里插入图片描述](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzg2MTQ2ZjhhMmFhYTQ2NGNhZjhlOTAwZjU2YmQ1MzUyLnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTYjcGljX2NlbnRlcg==)

### 广度优先性能

当时使用广度优先时，根据上面的硬件队列配合上面GPU队列调度器原理可以发现如下的结果，这样就实现了真正的三个流都并行了

![在这里插入图片描述](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuL2QyMjU3YmI5NWJjMjQ3NmM4ODJhMTNmNDA3MGFjZDI2LnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTYjcGljX2NlbnRlcj0xMHgxMA==)

### 多硬件队列

最早起的fermi架构的GPU可能就是一个硬件队列，现在生活富裕了，一般的GPU都支持32个硬件队列，但是实际上会默认只开8个硬件队列，所以使用的时候需要自己根据需求去设置`CUDA_DEVICE_MAX_CONNECTIONS`。这里我们假设有三个硬件队列，那么上面的结果就会如下：

![在这里插入图片描述](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzRkZDFhYTZhZmRlNzRlYzQ5ZTgxODYyZDg4NGQ1MjAyLnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTYjcGljX2NlbnRlcg==)

GPU队列调度器首先去队列1拿取C, 发现还有资源，然后去队列2拿取Q, 发现还有计算资源剩余，然后去队列3拿取Z,结果发现还有计算资源剩余，然后返回队1去拿取B, 但是B要依赖C, 只有C计算返回后才能拿，所以调度器就会等待，最后实现了三路并行。

![在这里插入图片描述](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzFiODRiYmNmNGI3NjRlMmE4ZjQwM2M1ZDFlMjM5MTk5LnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTYjcGljX2NlbnRlcg==)

## stream->quene

那么流中的任务是怎么映射到硬件队列呢？原理如下：根据实际的加入时间，比如首先是C-1(代表任务c, stream为1号)，映射的话首先看哪个硬件队列有1号stream, 如果有就放入该队列，没有的话就放在第一个序号为空的队列，比如1,2队列都有任务，3，4都为空，那么就把这个任务放在3号队列。
例子：加入只有两个GPU硬件队列，采用深度优先的加入方式话，那么结果如下：

![在这里插入图片描述](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuL2VhOWZkZmYxZGQ4YTRiYTNhYWNkYTViMTRmM2Q5OTdmLnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTYjcGljX2NlbnRlcg==)

​																							例子二：如果采用广度优先的话，队列如下：

![在这里插入图片描述](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzYzZTE4MzU1MjZkNDQwOWJiMWUwYjhjZTU5OGYwNWQ3LnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTYjcGljX2NlbnRlcg==)

### 个人建议

搞不明白的话，最好把队列设置的大于stream, 但是开设的队列越多占用的资源也多，搞得明白的话就学着用广度优先的添加方式。

### 代码示例

```c
#include "../common/common.h"
#include <stdio.h>
#include <cuda_runtime.h>
#include <stdlib.h>
#define N 30000000
#define M 1
#define NSTREAM 4

__global__ void kernel_1(double* res)
{
    
      
    double sum = 0.0;
    for(int j =0; j<M; j++)
    {
    
        for(int i = 0; i < N; i++)
        {
        sum = sum + tan(0.1) * tan(0.1);
        sum = sum + sin(0.1) * tan(0.1);
        sum = sum + cos(0.1) * tan(0.1);
        }
    }
    *res = sum;
}
nt main(int argc, char **argv)
{
    
      
    int n_streams = NSTREAM;
    int isize = 1;
    int iblock = 1;
    int bigcase = 0;

    // get argument from command line
    if (argc > 1) n_streams = atoi(argv[1]);

    if (argc > 2) bigcase = atoi(argv[2]);

    float elapsed_time;

    // set up max connectioin
    char* iname = "CUDA_DEVICE_MAX_CONNECTIONS";
    setenv (iname, "32", 1);
    char *ivalue =  getenv (iname);
    printf ("%s = %s\n", iname, ivalue);

    int dev = 0;
    cudaDeviceProp deviceProp;
    CHECK(cudaGetDeviceProperties(&deviceProp, dev));
    printf("> Using Device %d: %s with num_streams=%d\n", dev, deviceProp.name,
           n_streams);
    CHECK(cudaSetDevice(dev));

    // check if device support hyper-q
    if (deviceProp.major < 3 || (deviceProp.major == 3 && deviceProp.minor < 5))
    {
      
        if (deviceProp.concurrentKernels == 0)
        {
     
            printf("> GPU does not support concurrent kernel execution (SM 3.5 "
                    "or higher required)\n");
            printf("> CUDA kernel runs will be serialized\n");
        }
        else
        {  
            printf("> GPU does not support HyperQ\n");
            printf("> CUDA kernel runs will have limited concurrency\n");
       }
    }

    printf("> Compute Capability %d.%d hardware with %d multi-processors\n",
           deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);
    size_t nBytes = sizeof(double);
    double *d_A;
    CHECK(cudaMalloc((double**)&d_A, nBytes));

    // Allocate and initialize an array of stream handles
    cudaStream_t *streams = (cudaStream_t *) malloc(n_streams * sizeof(
                                cudaStream_t));

    for (int i = 0 ; i < n_streams ; i++)
    {
    
      
        CHECK(cudaStreamCreate(&(streams[i])));
    }

    // run kernel with more threads
    if (bigcase == 1)
    {
    
        iblock = 512;
        isize = 1 << 12;
    }

    // set up execution configuration
    dim3 block (iblock);
    dim3 grid  (isize / iblock);
    printf("> grid %d block %d\n", grid.x, block.x);

    // creat events
    cudaEvent_t start, stop;
    CHECK(cudaEventCreate(&start));
    CHECK(cudaEventCreate(&stop));

    // record start event
    CHECK(cudaEventRecord(start, 0));

    // dispatch job with depth first ordering
    for (int i = 0; i < n_streams; i++)
    {
    
      
        kernel_1<<<grid, block, 0, streams[i]>>>(d_A);
        kernel_2<<<grid, block, 0, streams[i]>>>(d_A);
        kernel_3<<<grid, block, 0, streams[i]>>>(d_A);
        kernel_4<<<grid, block, 0, streams[i]>>>(d_A);
    }

    // record stop event
    CHECK(cudaEventRecord(stop, 0));
    CHECK(cudaEventSynchronize(stop));

    // calculate elapsed time
    CHECK(cudaEventElapsedTime(&elapsed_time, start, stop));
    printf("Measured time for parallel execution = %.3fs\n",
           elapsed_time / 1000.0f);

    // release all stream
    for (int i = 0 ; i < n_streams ; i++)
    {
        CHECK(cudaStreamDestroy(streams[i]));
    }

    free(streams);

    // destroy events
    CHECK(cudaEventDestroy(start));
    CHECK(cudaEventDestroy(stop));

    // reset device
    CHECK(cudaDeviceReset());

    return 0;
}
```

### 结果

![在这里插入图片描述](http://140.238.201.79/image/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ19fc2h1YWkvYXJ0aWNsZS9kZXRhaWxzLzEyMjQ0MTAxMQ==_aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzRkNDY5YWNkYWRiZDQ1NGZhYWYxZmY5OTk1NjI0OTJmLnBuZz94LW9zcy1wcm9jZXNzPWltYWdlL3dhdGVybWFyayx0eXBlX2QzRjVMWHBsYm1obGFRLHNoYWRvd181MCx0ZXh0X1ExTkVUaUJBY3k1bVpXNW4sc2l6ZV8yMCxjb2xvcl9GRkZGRkYsdF83MCxnX3NlLHhfMTY=)

### 思考

- 在根据《CUDA C编程权威指南》提供的源码进行复现的时候，始终无法得到上述的结果，折腾两天后发现原来的源码中核函数没有输入和输出，这样在编译器优化的时候，直接不予计算了，所以怎么测都有问题，调整后就可以得到上述结果。
- 当每个kernel启动的时候，**grid** 和 **block** 的值都设置的很大的时候，并行的情况也不好，因为单个kernel执行的时候已经把计算资源用的差不多了，所以有时候资源调度器直接放弃取下个任务了。

```c

__global__
__device__
extern __share__ int tile[]
__shared__
__constant__

half
half2
float4
int4
__float22half2_rn()
    
cudaMemcpySymbol()
cudaFuncSetAttribute() // 设置shared memory 大小
```

------



## OPENCL

* 实时编译，因此可以适应多种异构平台

* 执行模式有任务并行和数据并行

  * 数据并行指的是不同的工作组执行同一个指令不同数据，通过`get_global_id` 来嵌入使用

  * 任务并行，多用于不同的 **kernel** 函数，以及队列命令乱序执行，当前命令没有对之前命令的依赖关系

    * 有序执行 (**in-order execution**)：命令按其在命令队列中出现的顺序发出，并按顺序完成。队列中前一个命令完成之后，下一个命令才会开始。这会将队列中命令的执行顺序串行化

    * 乱序执行 (**out-of-order execution**)：命令按顺序发出，但是下一个命令执行之前不会等待前一个命令完成。程序员要通过显式的同步机制来强制顺序

      

* **work-item**（工作项）：**work-item** 与 **cuda threads** 是一样的，是最小的执行单元。每次一个Kernel开始执行，很多（程序员定义数量）的 **work-item** 就开始运行，每个都执行同样的代码

  * 每个 **work-item** 有一个 id ，这个 id 在 kernel 中是可以访问的，每个运行在 **work-item **上的 kernel 通过这个 id 来找出**work-item**需要处理的数据

  * **work-item** 是循环中的最小单位

  * **work-item** 应该是由opencl自动分配给硬件线程的

    

* 存储器模式是软件定义的，而非单纯硬件实现；可以使用`constant` 来将数据放入常量存储区，访问更快

  * **Global memory:**工作区内的所有工作节点都可以自由的读写其中的任何数据。OpenCL C语言提供了全局缓存（Global buffer）的内建函数

  * **Constant memory:** 工作区内的所有工作节点可以读取其中的任何数据但不可以对数据内容进行更改，在内核程序的执行过程中保持不变。主机端负责分配和初始化常量缓存（Constant buffer）

  * **Local memory:** 只有同一工作组中的工作节点才可以对该类内存进行读写操作。它既可以为 OpenCL 的执行分配一块私有内存空间，也可以直接将其映射到一块全局缓存（**Global buffer**）上，特点是运行速度快

  * **Private memory:** 只有当前的工作节点( **item** )能对该内存进行访问和读写操作。一个工作节点内部的私有缓存（Private buffer）对其他节点来说是不可见的

    

* **CU** 对应cuda中的 **SM**，PE是SM中的一个处理单元

  * 一个work group的最大work item个数是指一个compute unit最多能调度、分配的线程数。这个数值一般就是一个CU内所包含的PE的个数的倍数。比如，如果一个GPU有2个CU，每个CU含有8个PE，而Max work group size是512，那么说明一个CU至少可以分配供512个线程并发操作所需要的各种资源。由于一个GPU根据一条算术逻辑指令能对所有PE发射若干次作为一个“原子的”发射操作，因此，这一个对程序员而言作为“原子的”发射操作启动了多少个线程，那么我们就可以认为是该GPU的最小并行线程数。如果一款GPU的最小线程并行数是32，那么该GPU将以32个线程作为一组原子的线程组。这意味着，如果遇到分支，那么一组32个线程组中的所有线程都将介入这个分支，对于不满足条件的线程，则会等到这32个线程中其它线程都完成分支处理之后再一起执行下面的指令。

    如果我将work group size指定为64，并且在kernel程序里加一个判断，如果pid小于32做操作A，否则做操作B，那么pid为0~31的线程组会执行操作A，而pid为32到63的线程组不会受到阻塞，而会立马执行操作B。此时，两组线程将 **并发** 操作（注意，这里是并发，而不是并行。因为上面讲过，GPU一次发射32个线程的话，那么对于多个32线程组将会调度发射指令）

  * 一个 work-item 不能被拆分到多个PE上处理；同样，一个 work-group 也不能拆分到多个CU上同时处理

  * 如果我想让 group 数量小点，那work-item的数目就会很多，还能不能处理了呢？以当前这个示例是能的，但是对于多的work-item,这涉及到如何确定work-item数目的问题

    结合cuda的概念进行解释：因为实际上，一个 SM 可以允许的 block 数量，还要另外考虑到他所用到 SM 的资源：shared memory、registers 等。在 G80 中，每个 SM 有 16KB 的 shared memory 和 8192 个 register。而在同一个 SM 里的 block 和 thread，则要共享这些资源;如果资源不够多个 block 使用的话，那 CUDA 就会减少 Block 的量，来让资源够用。在这种情形下，也会因此让 SM 的 thread 数量变少，而不到最多的 768 个

    比如说如果一个 thread 要用到 16 个 register 的话(在 kernel 中宣告的变量)，那一个 SM 的 8192 个 register 实际上只能让 512 个 thread 来使用;而如果一个 thread 要用 32 个 register，那一个 SM 就只能有 256 个 thread 了～而 shared memory 由于是 thread block 共享的，因此变成是要看一个 block 要用多少的 shared memory、一个 SM 的 16KB 能分给多少个 block 了

    所以虽然说当一个 SM 里的 thread 越多时，越能隐藏 latency，但是也会让每个 thread 能使用的资源更少。因此，这点也就是在优化时要做取舍的了

    

* kernel函数中间可以嵌套kernel函数，设备端可以创建命令队列，极大地方便了代码编写

  

  ````c
  #include <iostream>
  #include <stdlib.h>
  #include <string.h>
  #include <stdio.h>
   
   
  #if defined(__APPLE__) || defined(__MACOSX)
  #include <OpenCL/cl.hpp>
  #else
  #include <CL/cl.h>
  #endif
   
  using namespace std;
   
  #define KERNEL(...)#__VA_ARGS__
   
  const char *kernelSourceCode = KERNEL(
                                     __kernel void hellocl(__global uint *buffer)
  {
      size_t gidx = get_global_id(0);
      size_t gidy = get_global_id(1);
      size_t lidx = get_local_id(0);
      buffer[gidx + 4 * gidy] = (1 << gidx) | (0x10 << gidy);
   
  }
                                 );
  
  int main(int argc, char const *argv[])
  {
      printf("hello OpenCL\n");
      cl_int status = 0;
      size_t deviceListSize;
   
      // 当前服务器上配置的仅有NVIDIA Tesla C2050 的GPU
      cl_platform_id platform = NULL;
      status = clGetPlatformIDs(1, &platform, NULL);
   
      if (status != CL_SUCCESS) {
          printf("ERROR: Getting Platforms.(clGetPlatformIDs)\n");
          return EXIT_FAILURE;
      }
   
      // 如果我们能找到相应平台，就使用它，否则返回NULL
      cl_context_properties cps[3] = {
          CL_CONTEXT_PLATFORM,
          (cl_context_properties)platform,
          0
      };
   
      cl_context_properties *cprops = (NULL == platform) ? NULL : cps;
   
   
      // 生成 context
      cl_context context = clCreateContextFromType(
                               cprops,
                               CL_DEVICE_TYPE_GPU,
                               NULL,
                               NULL,
                               &status);
      if (status != CL_SUCCESS) {
          printf("Error: Creating Context.(clCreateContexFromType)\n");
          return EXIT_FAILURE;
      }
   
      // 寻找OpenCL设备
   
      // 首先得到设备列表的长度
      status = clGetContextInfo(context,
                                CL_CONTEXT_DEVICES,
                                0,
                                NULL,
                                &deviceListSize);
      if (status != CL_SUCCESS) {
          printf("Error: Getting Context Info device list size, clGetContextInfo)\n");
          return EXIT_FAILURE;
      }
      cl_device_id *devices = (cl_device_id *)malloc(deviceListSize);
      if (devices == 0) {
          printf("Error: No devices found.\n");
          return EXIT_FAILURE;
      }
   
      // 现在得到设备列表
      status = clGetContextInfo(context,
                                CL_CONTEXT_DEVICES,
                                deviceListSize,
                                devices,
                                NULL);
      if (status != CL_SUCCESS) {
          printf("Error: Getting Context Info (device list, clGetContextInfo)\n");
          return EXIT_FAILURE;
      }
   
   
      // 装载内核程序，编译CL program ,生成CL内核实例
   
      size_t sourceSize[] = {strlen(kernelSourceCode)};
      cl_program program = clCreateProgramWithSource(context,
                           1,
                           &kernelSourceCode,
                           sourceSize,
                           &status);
      if (status != CL_SUCCESS) {
          printf("Error: Loading Binary into cl_program (clCreateProgramWithBinary)\n");
          return EXIT_FAILURE;
      }
   
      // 为指定的设备编译CL program.
      status = clBuildProgram(program, 1, devices, NULL, NULL, NULL);
      if (status != CL_SUCCESS) {
          printf("Error: Building Program (clBuildingProgram)\n");
          return EXIT_FAILURE;
      }
   
      // 得到指定名字的内核实例的句柄
      cl_kernel kernel = clCreateKernel(program, "hellocl", &status);
      if (status != CL_SUCCESS) {
          printf("Error: Creating Kernel from program.(clCreateKernel)\n");
          return EXIT_FAILURE;
      }
   
      // 创建 OpenCL buffer 对象
      unsigned int *outbuffer = new unsigned int [4 * 4];
      memset(outbuffer, 0, 4 * 4 * 4);
      cl_mem outputBuffer = clCreateBuffer(
          context, 
          CL_MEM_ALLOC_HOST_PTR, 
          4 * 4 * 4, 
          NULL, 
          &status);
   
      if (status != CL_SUCCESS) {
          printf("Error: Create Buffer, outputBuffer. (clCreateBuffer)\n");
          return EXIT_FAILURE;
      }
   
      //  为内核程序设置参数
      status = clSetKernelArg(kernel, 0, sizeof(cl_mem), (void *)&outputBuffer);
      if (status != CL_SUCCESS) {
          printf("Error: Setting kernel argument. (clSetKernelArg)\n");
          return EXIT_FAILURE;
      }
   
      // 创建一个OpenCL command queue
      cl_command_queue commandQueue = clCreateCommandQueue(context,
                                      devices[0],
                                      0,
                                      &status);
      if (status != CL_SUCCESS) {
          printf("Error: Create Command Queue. (clCreateCommandQueue)\n");
          return EXIT_FAILURE;
      }
   
   
      // 将一个kernel 放入 command queue
      size_t globalThreads[] = {4, 4};
      size_t localThreads[] = {2, 2};
      status = clEnqueueNDRangeKernel(commandQueue, kernel,
                                      2, NULL, globalThreads,
                                      localThreads, 0,
                                      NULL, NULL);
      if (status != CL_SUCCESS) {
          printf("Error: Enqueueing kernel\n");
          return EXIT_FAILURE;
      }
   
      // 确认 command queue 中所有命令都执行完毕
      status = clFinish(commandQueue);
      if (status != CL_SUCCESS) {
          printf("Error: Finish command queue\n");
          return EXIT_FAILURE;
      }
   
      // 将内存对象中的结果读回Host
      status = clEnqueueReadBuffer(commandQueue,
                                   outputBuffer, CL_TRUE, 0,
                                   4 * 4 * 4, outbuffer, 0, NULL, NULL);
      if (status != CL_SUCCESS) {
          printf("Error: Read buffer queue\n");
          return EXIT_FAILURE;
      }
   
      // Host端打印结果
      printf("out:\n");
      for (int i = 0; i < 16; ++i) {
          printf("%x ", outbuffer[i]);
          if ((i + 1) % 4 == 0)
              printf("\n");
      }
   
      // 资源回收
      status = clReleaseKernel(kernel);
      status = clReleaseProgram(program);
      status = clReleaseMemObject(outputBuffer);
      status = clReleaseCommandQueue(commandQueue);
      status = clReleaseContext(context);
   
      free(devices);
      delete outbuffer;
      return 0;
  }
  ````



* 管道可以实现不同内核函数的交互
