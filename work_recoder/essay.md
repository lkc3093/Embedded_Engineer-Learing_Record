# **ML**
1. *relu* 优点
    * 当 *loss* 过大或者过小，*sigmoid*，*tanh* 的导数接近于0，*relu* 为非饱和激活函数不存在这种现象

2. resnet优点
   * 防止梯度消失

3. 数据预处理

4. *Batch normalization* 用于将数据的分布重整，标准化；数据变成正态分布，有效减少梯度消失

5. 动态调节学习率

6. *FSMN* 与 *RNN* 不同的是，每一层都保存的是前面时刻的feature，然后进行降维，输入到下一时刻的隐藏层中
  * *CFSMN* 将前一刻的feature降维了，减少参数；并只输出记忆模块参数
  * *DFSMN* 增加了跳跃输入，类似于resnet

------



## **算法优化**
* 深度可分离卷积，每个通道单独卷积，然后多个通道逐点点积
  * *MobileNet*
  
* 算子融合，前一个计算图的结果是后一个计算图的输入，可以融合，减少数据的 *load*，*store* 的操作
  * 可以通过 *tvm* 前端来实现
  
* 通过加 *L2* 正则，将输入的权重稀疏化，节省内存
  * 稀疏矩阵可以只访问非0值部分，减少指令读取的执行
  
* 向量指令加速
  * 内联，减少函数跳转，和堆栈开辟
  * 指令并行，无数据冲突的指令流水线执行
  * VILW，相当于多发射，指令槽
  
* 异构多核
  * 完成会报中断，类似于线程同步
  
* 量化
  * 定点数，int8
  * tensorflow训练中量化
  * 非对称量化更好
  
  ------
  
  ## RTOS
  
  * 线程切换类似于中断中的上下文保护
  * 互斥量是对共享内存(全局变量之类)的保护，只有`0 / 1` 状态
  * 信号量用于多线程同步
    * 线程获取一次信号量，信号量的值就会减1，当信号量的值减到0，再有线程获取信号量时，该线程就会被挂起到信号量的等待队列中，等待其他线程释放信号量
  
  
  * 由于在多处理器环境中某些资源的有限性，有时需要互斥访问(`mutual exclusion`)，这时候就需要引入锁的概念，只有获取了锁的线程才能够对资源进行访问，由于多线程的核心是CPU的时间分片，所以同一时刻只能有一个线程获取到锁。那么就面临一个问题，那么没有获取到锁的线程应该怎么办？
    * 通常有两种处理方式：一种是没有获取到锁的线程就一直循环等待判断该资源是否已经释放锁，这种锁叫做 `自旋锁`，它不用将线程阻塞起来(`NON-BLOCKING)`；还有一种处理方式就是把自己阻塞起来，等待重新调度请求，这种叫做`互斥锁`
  * 管道可以用于通信
  * sizeof 指针是4个字节
  * 1. 结构体变量的首地址能够被其最宽基本类型成员的大小所整除；
     2. 结构体每个成员相对结构体首地址的偏移量(offset)都是成员大小的整数倍，如有需编译器会在成员之间加上填充字节(internal adding)；
     3. 结构体的总大小为结构体最宽基本类型成员大小的整数倍，如有需要编译器会在最末一个成员之后加上填充字节{trailing padding}。
  
  ------
  
  

## TVM

### FrontEnd

  支持主流的深度学习前端框架，包括TensorFlow, MXNet, PyTorch, Keras, CNTK。目前TVM可以继承到PyTorch框架中优化、训练，而不是单纯地调用CNN模型接口

### Relay

  根据具体硬件对原始计算图进行重构、张量优化、数据重排等图优化操作。源代码分析中，Relay层比较杂，干的事情比较多，既对接上层的图优化又对接硬件的调度器



![img](https://iostream.io/wp-content/uploads/2019/09/Relay.png)



![img](https://iostream.io/wp-content/uploads/2019/09/Tensorlization.png)

​																									Relay及Tensorlization示例



### BackEnd

后端支持ARM、CUDA/Metal/OpenCL及加速器VTA

1. 生成硬件所需指令流与数据打包
2. 一个CPU与VTA的交互式运行环境：包括driver、JIT

------



## 算力评估

* FLOPS的计算公式如下

```text
浮点运算能力 = 处理器核数 * 每周期浮点运算次数 * 处理器主频
```
